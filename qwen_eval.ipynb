{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf7e4d4",
   "metadata": {},
   "source": [
    "# Qwen Evaluation Notebook\n",
    "\n",
    "생성된 결과(JSON)를 `eval.py` 로직과 동일하게 F1 평가하고 요약을 확인할 수 있는 노트북입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6426985",
   "metadata": {},
   "source": [
    "## 1. 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285184fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "# 예시: outputs/feta_tab_qwen/qwen_visdmrag\n",
    "EVAL_DIR = ROOT / 'outputs' / 'feta_tab_qwen' / 'qwen_visdmrag'\n",
    "EVAL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b5627",
   "metadata": {},
   "source": [
    "## 2. 평가 함수 (eval.py와 동일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return normalize_answer(text).split()\n",
    "\n",
    "\n",
    "def calculate_f1(prediction, ground_truth):\n",
    "    prediction_counter = Counter(prediction)\n",
    "    ground_truth_counter = Counter(ground_truth)\n",
    "\n",
    "    true_positives = sum((prediction_counter & ground_truth_counter).values())\n",
    "    false_positives = sum(prediction_counter.values()) - true_positives\n",
    "    false_negatives = sum(ground_truth_counter.values()) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) else 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def evaluate_directory(path: Path):\n",
    "    records = []\n",
    "    for json_file in sorted(path.glob('*.json')):\n",
    "        with json_file.open() as fh:\n",
    "            data = json.load(fh)\n",
    "        pred = str(data.get('Answer') or data.get('answer') or '')\n",
    "        gt = str(data.get('gt_answer') or '')\n",
    "        if not pred or not gt:\n",
    "            score = None\n",
    "        else:\n",
    "            score = calculate_f1(word_tokenize(pred), word_tokenize(gt))\n",
    "        records.append({'file': json_file.name, 'f1': score})\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5702c",
   "metadata": {},
   "source": [
    "## 3. 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not EVAL_DIR.exists():\n",
    "    raise FileNotFoundError(f'EVAL_DIR not found: {EVAL_DIR}')\n",
    "\n",
    "results_df = evaluate_directory(EVAL_DIR)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d034be",
   "metadata": {},
   "source": [
    "## 4. 통계 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = results_df.dropna(subset=['f1'])\n",
    "avg_f1 = valid_df['f1'].mean() if not valid_df.empty else float('nan')\n",
    "count = len(valid_df)\n",
    "print(f'Average F1: {avg_f1:.4f} over {count} files')\n",
    "valid_df.describe()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
